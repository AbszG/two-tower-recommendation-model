{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1cZebDYNo7e4Q3zmb-6HxAzok1wxvWneq",
      "authorship_tag": "ABX9TyPIUg/DZlw75Hde050AwEyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbszG/two-tower-recommendation-model/blob/main/Two_Tower_Recommendation_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0NY1M2EGNSCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzLfRHK9GzN5",
        "outputId": "2cb1b6b2-6e52-4c89-8115-a88a4bbaa71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load (likely expired) https://storage.googleapis.com/kaggle-data-sets/3748828/6487747/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240719%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240719T035436Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=13e682cd798685059a947e2b98ab3c6b66502b33042fa626cdbcea098cda185247d29cf36d76f375b237c920f7c6de7412e4514f4d0510a1f23ac18fb4ec77587a9f162ac03dabf966b869e903caa12a4c55c3226437dd46a3b3cca2a0aa385bb574bca7b20ac07fbd6f7722d7eba2aa35536fda7c634bbaaa96317672c3443a1698ff5f2d8b4f32336a6141abedb602c5354d9097e2a263b5629972780fd8f2b2f0fef43af74516d11ff2fdf86f62a11fcf518ca7ff53d22176130beeb4961d5d02a9e1b154aa438a382299d713deaa4de283455a9ed3d01376a26d385b1ab3fd572a4d98d572085c4d98a8753fff8a9aa0a3d14731b63367a4f136e72bd037 to path /kaggle/input/amazon-query-product-search\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'amazon-query-product-search:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3748828%2F6487747%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240719%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240719T035436Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D13e682cd798685059a947e2b98ab3c6b66502b33042fa626cdbcea098cda185247d29cf36d76f375b237c920f7c6de7412e4514f4d0510a1f23ac18fb4ec77587a9f162ac03dabf966b869e903caa12a4c55c3226437dd46a3b3cca2a0aa385bb574bca7b20ac07fbd6f7722d7eba2aa35536fda7c634bbaaa96317672c3443a1698ff5f2d8b4f32336a6141abedb602c5354d9097e2a263b5629972780fd8f2b2f0fef43af74516d11ff2fdf86f62a11fcf518ca7ff53d22176130beeb4961d5d02a9e1b154aa438a382299d713deaa4de283455a9ed3d01376a26d385b1ab3fd572a4d98d572085c4d98a8753fff8a9aa0a3d14731b63367a4f136e72bd037'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nM5JfyylJU7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "ismmd5c2JdeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/kaggle/input/amazon-query-product-search/Dataset.csv')\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4GC3zl8xIXU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chunk = df.sample(150000 , random_state = 42).reset_index(drop=True)\n",
        "df_chunk.head()"
      ],
      "metadata": {
        "id": "P9eWQGtJIXQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_chunk.shape)"
      ],
      "metadata": {
        "id": "EZOIUAEbIXNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_products = pd.read_parquet('/kaggle/input/amazon-query-product-search//shopping_queries_dataset_products.parquet')\n",
        "df_products.head()"
      ],
      "metadata": {
        "id": "0NkqK7GCIXLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['product_title', 'product_description', 'product_bullet_point','product_brand','product_color','product_id']\n",
        "df_products = pd.merge(df_products[cols].drop_duplicates(), df_chunk[['product_id']].drop_duplicates() ,on = ['product_id']) #merging to the shape of my dataset (150k)"
      ],
      "metadata": {
        "id": "7S5U2CtIKpuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_products.shape)\n",
        "\n",
        "df_products.head()"
      ],
      "metadata": {
        "id": "CSS4_TE-KrGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_products.isnull().sum())"
      ],
      "metadata": {
        "id": "E2KUIBiiLSrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lowercasing and handling null values\n",
        "df_products['product_title'] = df_products['product_title'].apply(lambda x : str(x).lower() if pd.notna(x) else '')\n",
        "df_products['product_description'] = df_products['product_description'].apply(lambda x : str(x).lower() if pd.notna(x) else '')\n",
        "df_products['product_bullet_point'] = df_products['product_bullet_point'].apply(lambda x : str(x).lower() if pd.notna(x) else '')\n",
        "df_products['product_brand'] = df_products['product_brand'].apply(lambda x : str(x).lower() if pd.notna(x) else '')\n",
        "df_products['product_color'] = df_products['product_color'].apply(lambda x : str(x).lower() if pd.notna(x) else '')\n",
        "df_chunk['query'] = df_chunk['query'].apply(lambda x : str(x).lower() if pd.notna(x) else '')"
      ],
      "metadata": {
        "id": "ng6DG4KMMEZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count words in a text\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "# Calculate median word count for each column\n",
        "median_title_length = df_products['product_title'].apply(count_words).median()\n",
        "median_description_length = df_products['product_description'].apply(count_words).median()\n",
        "median_bullet_point_length = df_products['product_bullet_point'].apply(count_words).median()\n",
        "median_brand_length = df_products['product_brand'].apply(count_words).median()\n",
        "median_color_length = df_products['product_color'].apply(count_words).median()\n",
        "median_query_length = df_chunk['query'].apply(count_words).median()\n",
        "\n",
        "print(\"Median Number of Words in 'query':\", median_query_length)\n",
        "print(\"Median Number of Words in 'product_title':\", median_title_length)\n",
        "print(\"Median Number of Words in 'product_description':\", median_description_length)\n",
        "print(\"Median Number of Words in 'product_bullet_point':\", median_bullet_point_length)\n",
        "print(\"Median Number of Words in 'product_brand':\", median_brand_length)\n",
        "print(\"Median Number of Words in 'product_color':\", median_color_length)"
      ],
      "metadata": {
        "id": "kNzuiM_sMYrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating unique query embeddings"
      ],
      "metadata": {
        "id": "LQl0kGbpMnia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = list(df_chunk['query'].unique())\n",
        "print(len(queries))"
      ],
      "metadata": {
        "id": "qyg79RfQMYpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_max_words = 10\n",
        "product_title_max_words = 50\n",
        "product_description_max_words = 20\n",
        "product_bullet_point_max_words = 50\n",
        "product_brand_max_words = 5\n",
        "product_color_max_words = 5"
      ],
      "metadata": {
        "id": "qyUUnsk3My3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reshaping sentence_transformers embeddings (768d to 160d/32d)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def reshape_array(input_array, d):\n",
        "    k, _ = input_array.shape\n",
        "    new_array = np.zeros((k, d))\n",
        "\n",
        "    for i in range(k):\n",
        "        for j in range(d):\n",
        "            start_idx = j * (768 // d)\n",
        "            end_idx = (j + 1) * (768 // d) if j < (d - 1) else 768\n",
        "            chunk = input_array[i, start_idx:end_idx]\n",
        "            new_array[i, j] = np.mean(chunk)\n",
        "\n",
        "    return new_array\n",
        "\n",
        "\n",
        "def find_embeddings(lst_product_title,i, maxlen):\n",
        "    # Define a list of sentences\n",
        "    sentences = list(lst_product_title)[i:i+step]\n",
        "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "    sentence_embeddings = sentence_embeddings.cpu().numpy()\n",
        "\n",
        "    return reshape_array(np.array(sentence_embeddings), product_dim)"
      ],
      "metadata": {
        "id": "VH9afB8RNQdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-uncased' ,  device=\"cuda\")"
      ],
      "metadata": {
        "id": "pFmkvvlMNQMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 1000\n",
        "query_dim = 32\n",
        "\n",
        "cols = ['q' + str(x) for x in list(range(0, query_dim))] + ['query']\n",
        "cnt = 0\n",
        "\n",
        "for i in range(0,len(queries),step):\n",
        "\n",
        "    cnt += 1\n",
        "    # Define a list of sentences\n",
        "    sentences = list(queries)[i:i+step]\n",
        "\n",
        "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "    sentence_embeddings = sentence_embeddings.cpu().numpy()\n",
        "    sentence_embeddings = reshape_array(np.array(sentence_embeddings), query_dim)\n",
        "\n",
        "    df_tmp = pd.DataFrame(np.concatenate((sentence_embeddings, np.array(sentences).reshape(-1,1)), axis=1))\n",
        "\n",
        "    df_tmp.columns = cols\n",
        "\n",
        "    df_tmp.to_csv(\n",
        "        f'query_{cnt}.csv', header = True, index = False)\n",
        "\n",
        "    print(i)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2PB5kJ7CM0UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating unique product embeddings"
      ],
      "metadata": {
        "id": "LiHrDsDfPI_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lst_product_title = list(df_products['product_title'])\n",
        "lst_product_description = list(df_products['product_description'])\n",
        "lst_product_bullet_point = list(df_products['product_bullet_point'])\n",
        "lst_product_brand = list(df_products['product_brand'])\n",
        "lst_product_color = list(df_products['product_color'])\n",
        "lst_product_id = list(df_products['product_id'])"
      ],
      "metadata": {
        "id": "i8y2rOvxOiNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_product_title = []\n",
        "results_product_description = []\n",
        "results_product_bullet_point = []\n",
        "results_product_brand = []\n",
        "results_product_color = []\n",
        "\n",
        "step = 10000\n",
        "product_dim = 32\n",
        "\n",
        "cols = ['p' + str(x) for x in list(range(0, product_dim * 5))] + ['product_id']\\\n",
        "\n",
        "cnt = 0\n",
        "for i in range(0,len(lst_product_title),step):\n",
        "    cnt += 1\n",
        "    product_title_embed = find_embeddings(lst_product_title, i, product_title_max_words)\n",
        "    product_description_embed = find_embeddings(lst_product_description, i, product_description_max_words)\n",
        "    product_bullet_point_embed = find_embeddings(lst_product_bullet_point, i, product_bullet_point_max_words)\n",
        "    product_brand_embed = find_embeddings(lst_product_brand, i, product_brand_max_words)\n",
        "    product_color_embed = find_embeddings(lst_product_color, i, product_color_max_words)\n",
        "\n",
        "    # Concatenate arrays column-wise and reshape lst_product_id to (x, 1)\n",
        "    df_tmp = pd.DataFrame(np.concatenate((\n",
        "        product_title_embed,\n",
        "        product_description_embed,\n",
        "        product_bullet_point_embed,\n",
        "        product_brand_embed,\n",
        "        product_color_embed,\n",
        "        np.array(lst_product_id[i:i + step]).reshape(-1, 1)\n",
        "    ), axis=1))\n",
        "\n",
        "\n",
        "    df_tmp.columns = cols\n",
        "    df_tmp.to_csv(f'product_{cnt}.csv', header = True, index = False)\n",
        "\n",
        "\n",
        "    print(i)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sFyc8E_GOiK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenating embedding files\n"
      ],
      "metadata": {
        "id": "CZ9kfQ3HTlQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "file_list = glob.glob('product_*.csv')\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for file in file_list:\n",
        "    df = pd.read_csv(file)\n",
        "    dfs.append(df)\n",
        "\n",
        "concatenated_product_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "concatenated_product_df.to_csv('product.csv', index=False)\n",
        "\n",
        "print(concatenated_product_df.shape)\n",
        "\n",
        "concatenated_product_df.head()"
      ],
      "metadata": {
        "id": "txTSHQKPTYEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "\n",
        "file_list = glob.glob('query_*.csv')\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for file in file_list:\n",
        "    df = pd.read_csv(file)\n",
        "    dfs.append(df)\n",
        "\n",
        "concatenated_query_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "concatenated_query_df.to_csv('query.csv', index=False)\n",
        "\n",
        "print(concatenated_query_df.shape)\n",
        "\n",
        "concatenated_query_df.head()"
      ],
      "metadata": {
        "id": "wOWMiNOfTYBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chunk  = pd.merge(df_chunk, concatenated_query_df, on = 'query')\n",
        "df_chunk  = pd.merge(df_chunk, concatenated_product_df, on = 'product_id')\n",
        "\n",
        "print(df_chunk.shape)\n",
        "\n",
        "df_chunk.head()"
      ],
      "metadata": {
        "id": "tGMV-wooVgap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EiJuhZM9cyW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chunk.to_csv('/content/drive/MyDrive/AmazonDatasets/dataset_chunk.csv', header = True, index = False)"
      ],
      "metadata": {
        "id": "owYcTkroazph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_product_embedding = pd.merge(\n",
        "    pd.read_csv('product.csv'),\n",
        "    pd.read_parquet('/kaggle/input/amazon-query-product-search/shopping_queries_dataset_products.parquet')[['product_id','product_title']],\n",
        "    on = ['product_id']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "df_product_embedding['pid'] = range(0, df_product_embedding.shape[0])\n",
        "\n",
        "df_query_embedding = pd.read_csv('query.csv')\n",
        "df_query_embedding['qid'] = range(0, df_query_embedding.shape[0])"
      ],
      "metadata": {
        "id": "AB4H0vvTa1Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_query_embedding.head()"
      ],
      "metadata": {
        "id": "VIqi6V8ra1DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_product_embedding.head()"
      ],
      "metadata": {
        "id": "BKZx0sSpbOUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_query_embedding.shape, df_product_embedding.shape)"
      ],
      "metadata": {
        "id": "saVtrLVJbORp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_product_embedding.to_csv('/content/drive/MyDrive/AmazonDatasets/product_embedding_chunk.csv', header = True, index = False)\n",
        "df_query_embedding.to_csv('/content/drive/MyDrive/AmazonDatasets/query_embedding_chunk.csv', header = True, index = False)"
      ],
      "metadata": {
        "id": "2pXpf9cldd2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quality Checking (NNA)**"
      ],
      "metadata": {
        "id": "HCfY19AUd6BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "o2pl24Dpe2Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_tower_input_dim = 32\n",
        "product_tower_input_dim = (32*5) # for each product in 32d"
      ],
      "metadata": {
        "id": "xpiJ3-djd4aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from annoy import AnnoyIndex  #finding simillar queries\n",
        "q = AnnoyIndex(query_tower_input_dim, 'euclidean')\n",
        "mp_query_dict = {}\n",
        "\n",
        "for ix,row in df_query_embedding.iterrows():\n",
        "    mp_query_dict[row['qid']] = row['query']\n",
        "\n",
        "    key = int(row['qid'])\n",
        "    vec = list(row[['q'+str(x) for x in list(range(query_tower_input_dim))]])\n",
        "\n",
        "    q.add_item(key,vec)"
      ],
      "metadata": {
        "id": "fJeFnfZleTFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q.build(100) # 100 trees\n",
        "q.save('query.tree')\n",
        "\n",
        "\n",
        "q = AnnoyIndex(query_tower_input_dim,  'euclidean')\n",
        "q.load('query.tree')"
      ],
      "metadata": {
        "id": "mmZZ_X7FeXPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 20\n",
        "mat = []\n",
        "for ix,row in df_query_embedding.iterrows():\n",
        "    item = row['query']\n",
        "    mat.append([item] + [mp_query_dict[x] for x in q.get_nns_by_item(row['qid'], top_k+1)[1:]])\n",
        "\n",
        "    if ix == 50:\n",
        "        break\n",
        "\n",
        "cols = ['query_id']\n",
        "for i in range(top_k):\n",
        "    cols += ['nearest_{}'.format(i+1)]\n",
        "\n",
        "print(cols)\n",
        "\n",
        "df_neighbors1 = pd.DataFrame(mat, columns = cols)\n",
        "\n",
        "display(df_neighbors1.head(50)\n",
        "       )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tI4ovZaCecN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = AnnoyIndex(product_tower_input_dim, 'euclidean') #finding simillar products\n",
        "mp_product_dict = {}\n",
        "\n",
        "for ix,row in df_product_embedding.iterrows():\n",
        "    mp_product_dict[int(row['pid'])] = row['product_title']\n",
        "\n",
        "    key = int(row['pid'])\n",
        "    vec = list(row[['p'+str(x) for x in list(range(product_tower_input_dim))]])\n",
        "\n",
        "    p.add_item(key,vec)"
      ],
      "metadata": {
        "id": "9HJP0ZqJeebI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p.build(100) # 100 trees\n",
        "p.save('product.tree')\n",
        "\n",
        "\n",
        "p = AnnoyIndex(product_tower_input_dim,  'euclidean')\n",
        "p.load('product.tree')"
      ],
      "metadata": {
        "id": "nsMHxQkWerHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mp_product_dict.items())"
      ],
      "metadata": {
        "id": "ND1c-ATrewVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 20\n",
        "mat = []\n",
        "for ix,row in df_product_embedding.iterrows():\n",
        "    item = row['product_title']\n",
        "    mat.append([item] + [mp_product_dict[x] for x in p.get_nns_by_item(row['pid'], top_k+1)[1:]])\n",
        "\n",
        "    if ix == 50:\n",
        "        break\n",
        "\n",
        "cols = ['product_id']\n",
        "for i in range(top_k):\n",
        "    cols += ['nearest_{}'.format(i+1)]\n",
        "\n",
        "print(cols)\n",
        "\n",
        "df_neighbors2 = pd.DataFrame(mat, columns = cols)\n",
        "\n",
        "display(df_neighbors2.head(50))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "P1LjpQvyew8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "qnBj38DBgKTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Prep**"
      ],
      "metadata": {
        "id": "yFmSJBmeg4LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df_chunk = pd.merge(\n",
        "    pd.read_csv('/content/drive/MyDrive/AmazonDatasets/dataset_chunk.csv'),\n",
        "    pd.read_parquet('/kaggle/input/amazon-query-product-search/shopping_queries_dataset_products.parquet')[['product_id','product_title']],\n",
        "    on = ['product_id']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(df_chunk.shape)\n",
        "\n",
        "df_chunk.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "05XmbJs-hu-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_product_embedding = pd.read_csv('/content/drive/MyDrive/AmazonDatasets/product_embedding_chunk.csv')\n",
        "df_query_embedding = pd.read_csv('/content/drive/MyDrive/AmazonDatasets/query_embedding_chunk.csv')"
      ],
      "metadata": {
        "id": "jx3K7kski8NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chunk.esci_label.value_counts()"
      ],
      "metadata": {
        "id": "cyZ0qNP4jYfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chunk[df_chunk.esci_label == 'E'][['query','product_title','split']].sample(20)"
      ],
      "metadata": {
        "id": "vL4gg8P2jiWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chunk[df_chunk.esci_label == 'I'][['query','product_title','split']].sample(20)"
      ],
      "metadata": {
        "id": "PlGmbMoFjqOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([p for p in df_product_embedding.columns])\n",
        "\n",
        "print([p for p in df_query_embedding.columns])"
      ],
      "metadata": {
        "id": "ikhTFBEPj4Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting 'esci_label' to binary values\n",
        "df_chunk['binary_label'] = df_chunk['esci_label'].apply(lambda x: 1 if x == 'E' else 0)\n",
        "\n",
        "train_data = df_chunk[df_chunk['split'] != 'test']\n",
        "val_data = df_chunk[df_chunk['split'] == 'test']\n",
        "\n",
        "train_labels = np.array(train_data['binary_label'])\n",
        "val_labels = np.array(val_data['binary_label'])\n",
        "\n",
        "train_labels = train_labels.astype('float32')\n",
        "val_labels = val_labels.astype('float32')\n",
        "\n",
        "\n",
        "\n",
        "print(train_data.esci_label.value_counts(),\n",
        "      val_data.esci_label.value_counts())\n",
        "\n",
        "query_tower_cols = ['q0', 'q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q7', 'q8',\n",
        "                                  'q9', 'q10', 'q11', 'q12', 'q13', 'q14', 'q15', 'q16',\n",
        "                                  'q17', 'q18', 'q19', 'q20', 'q21', 'q22', 'q23', 'q24',\n",
        "                                  'q25', 'q26', 'q27', 'q28', 'q29', 'q30', 'q31']\n",
        "\n",
        "product_tower_cols = ['p0', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10',\n",
        "                      'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19', 'p20',\n",
        "                      'p21', 'p22', 'p23', 'p24', 'p25', 'p26', 'p27', 'p28', 'p29', 'p30',\n",
        "                      'p31', 'p32', 'p33', 'p34', 'p35', 'p36', 'p37', 'p38', 'p39', 'p40',\n",
        "                      'p41', 'p42', 'p43', 'p44', 'p45', 'p46', 'p47', 'p48', 'p49', 'p50',\n",
        "                      'p51', 'p52', 'p53', 'p54', 'p55', 'p56', 'p57', 'p58', 'p59', 'p60',\n",
        "                      'p61', 'p62', 'p63', 'p64', 'p65', 'p66', 'p67', 'p68', 'p69', 'p70',\n",
        "                      'p71', 'p72', 'p73', 'p74', 'p75', 'p76', 'p77', 'p78', 'p79', 'p80',\n",
        "                      'p81', 'p82', 'p83', 'p84', 'p85', 'p86', 'p87', 'p88', 'p89', 'p90',\n",
        "                      'p91', 'p92', 'p93', 'p94', 'p95', 'p96', 'p97', 'p98', 'p99', 'p100',\n",
        "                      'p101', 'p102', 'p103', 'p104', 'p105', 'p106', 'p107', 'p108', 'p109', 'p110',\n",
        "                      'p111', 'p112', 'p113', 'p114', 'p115', 'p116', 'p117', 'p118', 'p119', 'p120',\n",
        "                      'p121', 'p122', 'p123', 'p124', 'p125', 'p126', 'p127', 'p128', 'p129', 'p130',\n",
        "                      'p131', 'p132', 'p133', 'p134', 'p135', 'p136', 'p137', 'p138', 'p139', 'p140',\n",
        "                      'p141', 'p142', 'p143', 'p144', 'p145', 'p146', 'p147', 'p148', 'p149', 'p150',\n",
        "                      'p151', 'p152', 'p153', 'p154', 'p155', 'p156', 'p157', 'p158', 'p159'\n",
        "                     ]\n",
        "\n",
        "\n",
        "\n",
        "# Prepare input data for training and validation\n",
        "train_inputs = [\n",
        "    np.array(train_data[query_tower_cols]),\n",
        "\n",
        "    np.array(train_data[product_tower_cols])\n",
        "\n",
        "]\n",
        "\n",
        "val_inputs = [\n",
        "    np.array(val_data[query_tower_cols]),\n",
        "\n",
        "    np.array(val_data[product_tower_cols])\n",
        "]"
      ],
      "metadata": {
        "id": "jWG5MnjskrXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo torchviz\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FjJg3uXrx57O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Model**"
      ],
      "metadata": {
        "id": "Ea2Z0EsklXLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "\n",
        "embedding_dim = 16\n",
        "\n",
        "class TwoTowerModel(nn.Module):\n",
        "    def __init__(self, query_tower_input_dim, product_tower_input_dim, embedding_dim):\n",
        "        super(TwoTowerModel, self).__init__()\n",
        "        self.query_embedding = nn.Linear(query_tower_input_dim, embedding_dim)\n",
        "        self.product_embedding = nn.Linear(product_tower_input_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, input_query=None, input_product=None, return_query_embedding=False, return_product_embedding=False):\n",
        "        query_embed, product_embed = None, None\n",
        "\n",
        "        if input_query is not None:\n",
        "            query_embed = self.query_embedding(input_query)\n",
        "            if return_query_embedding:\n",
        "                return query_embed\n",
        "\n",
        "        if input_product is not None:\n",
        "            product_embed = self.product_embedding(input_product)\n",
        "            if return_product_embedding:\n",
        "                return product_embed\n",
        "\n",
        "        if query_embed is not None:\n",
        "            normalized_query = F.normalize(query_embed, p=2, dim=1)\n",
        "        if product_embed is not None:\n",
        "            normalized_product = F.normalize(product_embed, p=2, dim=1)\n",
        "\n",
        "        if query_embed is not None and product_embed is not None:\n",
        "            cosine_similarity = F.cosine_similarity(normalized_query, normalized_product, dim=1)\n",
        "            return cosine_similarity\n",
        "\n",
        "\n",
        "query_tower_input_dim = 32\n",
        "product_tower_input_dim = 5 * 32\n",
        "\n",
        "model = TwoTowerModel(query_tower_input_dim, product_tower_input_dim, embedding_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(summary(model, input_size=[(1, query_tower_input_dim), (1, product_tower_input_dim)]))\n",
        "\n"
      ],
      "metadata": {
        "id": "iZjHSO1XlDQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "sample_query = torch.randn(1, query_tower_input_dim, device=device)\n",
        "sample_product = torch.randn(1, product_tower_input_dim, device=device)\n",
        "y = model(sample_query, sample_product)\n",
        "\n",
        "dot = make_dot(y, params=dict(list(model.named_parameters())))\n",
        "\n",
        "dot.render(\"model_structure\", format=\"png\")\n",
        "\n",
        "img = mpimg.imread('model_structure.png')\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SkBglRQKn8r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    epoch_train_loss = 0\n",
        "    for i in range(0, len(train_data), batch_size):\n",
        "        batch_query = torch.tensor(train_inputs[0][i: i + batch_size]).float().to(device)\n",
        "        batch_product = torch.tensor(train_inputs[1][i: i + batch_size]).float().to(device)\n",
        "        batch_labels = torch.tensor(train_labels[i: i + batch_size]).float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_query, batch_product)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "    train_losses.append(epoch_train_loss / (len(train_data) // batch_size))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(val_data), batch_size):\n",
        "            batch_query = torch.tensor(val_inputs[0][i: i + batch_size]).float().to(device)\n",
        "            batch_product = torch.tensor(val_inputs[1][i: i + batch_size]).float().to(device)\n",
        "            batch_labels = torch.tensor(val_labels[i: i + batch_size]).float().to(device)\n",
        "\n",
        "            outputs = model(batch_query, batch_product)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            epoch_val_loss += loss.item()\n",
        "\n",
        "    val_losses.append(epoch_val_loss / (len(val_data) // batch_size))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]}, Validation Loss: {val_losses[-1]}\")\n"
      ],
      "metadata": {
        "id": "H45lKtu377tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/MyDrive/AmazonDatasets/twotower.pth'\n",
        "\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f'Model saved to {model_save_path}')"
      ],
      "metadata": {
        "id": "6-nZsHAe-0P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the Performance of the Model**"
      ],
      "metadata": {
        "id": "p71ePPPjFvs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print('Model loaded successfully')"
      ],
      "metadata": {
        "id": "u1gWWQxwEeT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding extraction\n"
      ],
      "metadata": {
        "id": "kFYrctTSrFCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install annoy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k7fMpUHj2UL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_product_embedding(model, input_product): #chopping product tower\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_product = torch.tensor(input_product).float().to(device)\n",
        "        product_embed = model(input_product=input_product, return_product_embedding=True)\n",
        "    return product_embed.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "product_tower_input_dim = 5 * 32\n",
        "\n",
        "input_data_product = np.array(df_product_embedding[product_tower_cols])\n",
        "\n",
        "product_embeddings = get_product_embedding(model, input_data_product)\n",
        "\n",
        "df_product_embeddings_model = pd.DataFrame(product_embeddings, columns=[f'p{x}' for x in range(embedding_dim)])\n",
        "df_product_embeddings_model['product_id'] = df_product_embedding['product_id']\n",
        "df_product_embeddings_model['product_title'] = df_product_embedding['product_title']\n",
        "df_product_embeddings_model['pid'] = df_product_embedding['pid']\n",
        "\n",
        "df_product_embeddings_model.head()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EyhmpMBiGSA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm = AnnoyIndex(embedding_dim, 'dot') #running ANN to check performance of product tower\n",
        "mp_product_dict = {}\n",
        "\n",
        "for ix,row in df_product_embeddings_model.iterrows():\n",
        "    mp_product_dict[int(row['pid'])] = row['product_title']\n",
        "\n",
        "    key = int(row['pid'])\n",
        "    vec = list(row[['p'+str(x) for x in list(range(embedding_dim))]])\n",
        "\n",
        "    pm.add_item(key,vec)"
      ],
      "metadata": {
        "id": "DN9c1N37JXCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.build(100) # 100 trees\n",
        "pm.save('product_model.tree')\n",
        "\n",
        "pm = AnnoyIndex(embedding_dim,  'dot')\n",
        "pm.load('product_model.tree')"
      ],
      "metadata": {
        "id": "YQHx7sJtJW_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 20\n",
        "mat = []\n",
        "for ix,row in df_product_embeddings_model.iterrows():\n",
        "    item = row['product_title']\n",
        "    mat.append([item] + [mp_product_dict[x] for x in pm.get_nns_by_item(row['pid'], top_k+1)[1:]])\n",
        "\n",
        "    if ix == 50:\n",
        "        break\n",
        "\n",
        "cols = ['product_id']\n",
        "for i in range(top_k):\n",
        "    cols += ['nearest_{}'.format(i+1)]\n",
        "\n",
        "print(cols)\n",
        "\n",
        "df_neighbors3 = pd.DataFrame(mat, columns = cols)\n",
        "\n",
        "display(df_neighbors3.head(50))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5wPKJ2ngJW8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_embedding(model, input_query): #chopping query tower\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_query = torch.tensor(input_query).float().to(device)\n",
        "        query_embed = model(input_query=input_query, return_query_embedding=True)\n",
        "    return query_embed.cpu().numpy()\n",
        "\n",
        "query_tower_input_dim = 32\n",
        "\n",
        "input_data_query = np.array(df_query_embedding[query_tower_cols])\n",
        "\n",
        "query_embeddings = get_query_embedding(model, input_data_query)\n",
        "\n",
        "df_query_embeddings_model = pd.DataFrame(query_embeddings, columns = [f'q{x}'for x in range(embedding_dim)] )\n",
        "\n",
        "df_query_embeddings_model['query'] = df_query_embedding['query']\n",
        "df_query_embeddings_model['qid'] = df_query_embedding['qid']\n",
        "\n",
        "df_query_embeddings_model.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-816tUspGR9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from annoy import AnnoyIndex #running ANN to check the performance of the query tower\n",
        "qm = AnnoyIndex(embedding_dim, 'dot')\n",
        "mp_query_dict = {}\n",
        "\n",
        "for ix,row in df_query_embeddings_model.iterrows():\n",
        "    mp_query_dict[row['qid']] = row['query']\n",
        "\n",
        "    key = int(row['qid'])\n",
        "    vec = list(row[['q'+str(x) for x in list(range(embedding_dim))]])\n",
        "\n",
        "    qm.add_item(key,vec)"
      ],
      "metadata": {
        "id": "Xi2uvpNtGRzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qm.build(100) # 100 trees\n",
        "qm.save('query_model.tree')\n",
        "\n",
        "qm = AnnoyIndex(embedding_dim,  'dot')\n",
        "qm.load('query_model.tree')"
      ],
      "metadata": {
        "id": "3PP3_bQ7LokV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 20\n",
        "mat = []\n",
        "for ix,row in df_query_embeddings_model.iterrows():\n",
        "    item = row['query']\n",
        "    mat.append([item] + [mp_query_dict[x] for x in qm.get_nns_by_item(row['qid'], top_k+1)[1:]])\n",
        "\n",
        "    if ix == 50:\n",
        "        break\n",
        "\n",
        "cols = ['query_id']\n",
        "for i in range(top_k):\n",
        "    cols += ['nearest_{}'.format(i+1)]\n",
        "\n",
        "print(cols)\n",
        "\n",
        "df_neighbors4 = pd.DataFrame(mat, columns = cols)\n",
        "\n",
        "display(df_neighbors4.head(50))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dGfgVe9H6UE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Random Inputs"
      ],
      "metadata": {
        "id": "-eNDK5bcMvhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_search_queries = [\n",
        "\n",
        "    \"Bluetooth earbuds under $50\",\n",
        "    \"Designer handbags on clearance\",\n",
        "    \"Gaming laptops with RTX graphics\",\n",
        "    \"Organic skincare products\",\n",
        "    \"Smartphones with long battery life\",\n",
        "    \"Running shoes for flat feet\",\n",
        "    \"Laptop deals for students\",\n",
        "    \"Wireless noise-canceling headphones\",\n",
        "    \"Digital cameras for beginners\",\n",
        "    \"Best coffee makers under $50\",\n",
        "    \"Latest fashion trends for summer\",\n",
        "    \"Top-rated gaming keyboards\",\n",
        "    \"Dining room furniture sets\",\n",
        "    \"Winter coats for kids\",\n",
        "    \"Men's watches with leather straps\",\n",
        "    \"Home gym equipment for small spaces\",\n",
        "    \"Women's handbags on sale\",\n",
        "    \"Bluetooth speakers with waterproof features\",\n",
        "    \"Outdoor patio furniture\",\n",
        "    \"Affordable fitness trackers\",\n",
        "     \"Red dress for a wedding\",\n",
        "    \"Best laptop under $1000\",\n",
        "    \"Nike running shoes for women\",\n",
        "    \"iPhone 13 reviews\",\n",
        "    \"Men's winter jackets on sale\",\n",
        "    \"Top-rated kitchen appliances\",\n",
        "    \"Summer dresses for women\",\n",
        "    \"Samsung TV specifications\",\n",
        "    \"Gift ideas for anniversary\",\n",
        "    \"XYZ brand headphones\",\n",
        "    \"Women's shoes size 8\",\n",
        "    \"Deals on smartphones\",\n",
        "    \"Men's formal suits\",\n",
        "    \"Fitness trackers with heart rate monitor\",\n",
        "    \"Digital cameras with 4K video\",\n",
        "    \"Wireless gaming mouse\",\n",
        "    \"Outdoor camping gear\",\n",
        "    \"Cookware sets for induction cooktops\",\n",
        "    \"Portable chargers for smartphones\",\n",
        "    \"Desktop computers for gaming\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "JJd72evJMO6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AJklsXkBM6Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model_sentence_transformers = SentenceTransformer('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "gAGh8SGOM6P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def reshape_array(input_array, d):\n",
        "    k, _ = input_array.shape\n",
        "    new_array = np.zeros((k, d))\n",
        "\n",
        "    for i in range(k):\n",
        "        for j in range(d):\n",
        "            start_idx = j * (768 // d)\n",
        "            end_idx = (j + 1) * (768 // d) if j < (d - 1) else 768\n",
        "            chunk = input_array[i, start_idx:end_idx]\n",
        "            new_array[i, j] = np.mean(chunk)\n",
        "\n",
        "    return new_array"
      ],
      "metadata": {
        "id": "auMu0d4rNFjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_max_words = 10\n",
        "product_title_max_words = 50\n",
        "product_description_max_words = 20\n",
        "product_bullet_point_max_words = 50\n",
        "product_brand_max_words = 5\n",
        "product_color_max_words = 5"
      ],
      "metadata": {
        "id": "2hi4B5O3NFg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_dim = 32\n",
        "\n",
        "cols = ['q' + str(x) for x in list(range(0, query_dim))] + ['query']\n",
        "\n",
        "sentences = product_search_queries\n",
        "\n",
        "sentence_embeddings = model_sentence_transformers.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "sentence_embeddings = reshape_array(sentence_embeddings.cpu().numpy(), query_dim)\n",
        "\n",
        "df_query_test_actual = pd.DataFrame(sentence_embeddings)\n",
        "\n",
        "df_query_test_actual.columns = query_tower_cols\n",
        "df_query_test_actual['query'] = product_search_queries\n",
        "\n",
        "df_query_test_actual.head(10)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DyYPMbw9NFeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embeddings"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sjNnWupoNiIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_embedding(model, input_query):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_query = torch.tensor(input_query).float().to(device)\n",
        "        query_embed = model(input_query=input_query, return_query_embedding=True)\n",
        "    return query_embed.cpu().numpy()\n",
        "\n",
        "print(f\"Original DataFrame shape: {df_query_test_actual.shape}\")\n",
        "\n",
        "input_data_query = np.array(df_query_test_actual[query_tower_cols])\n",
        "\n",
        "print(f\"Input data shape for query embeddings: {input_data_query.shape}\")\n",
        "\n",
        "query_embeddings = get_query_embedding(model, input_data_query)\n",
        "\n",
        "print(f\"Query embeddings shape: {query_embeddings.shape}\")\n",
        "\n",
        "assert query_embeddings.shape[0] == df_query_test_actual.shape[0], \"Mismatch in number of rows between input data and obtained embeddings\"\n"
      ],
      "metadata": {
        "id": "00OgQILkNiGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [f'q{x}' for x in range(embedding_dim)]\n",
        "df_query_test = pd.DataFrame(query_embeddings, columns=cols)\n",
        "df_query_test['query'] = product_search_queries\n",
        "\n",
        "df_query_test.head(50)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MTCKhXLcO7rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "top_k = 20\n",
        "\n",
        "desc = []\n",
        "for i in range(top_k):\n",
        "    desc.append(f'product_title_{(i+1)}')\n",
        "desc = ['query'] + desc\n",
        "\n",
        "for ix,row in df_query_test.iterrows():\n",
        "    qvec = [f'q{x}' for x in list(range(embedding_dim))]\n",
        "    qvec = row[qvec]\n",
        "    similar_vector_ids,similar_distances = pm.get_nns_by_vector(qvec, top_k, include_distances=True)\n",
        "\n",
        "    similar_vector_product_title = [mp_product_dict[x] for x in similar_vector_ids]\n",
        "    results.append([row['query']] + similar_vector_product_title)\n",
        "\n",
        "df_results = pd.DataFrame(results, columns =  desc)\n",
        "\n",
        "df_results.head(50)"
      ],
      "metadata": {
        "id": "FF3pO-XkPEQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}